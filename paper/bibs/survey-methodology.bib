@article{Zagheni2017,
author = {Zagheni, Emilio and Weber, Ingmar and Gummadi, Krishna},
doi = {10.1111/padr.12102},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zagheni, Weber, Gummadi - 2017 - Leveraging Facebook's Advertising Platform to Monitor Stocks of Migrants.pdf:pdf},
issn = {17284457},
journal = {Population and Development Review},
number = {4},
pages = {721--734},
title = {Leveraging {Facebook's} Advertising Platform to Monitor Stocks of Migrants},
volume = {43},
year = {2017}
}
@article{Lindhjem2011,
abstract = {Internet is quickly becoming the survey mode of choice for stated preference (SP) surveys in environmental economics. However, this choice is being made with relatively little consideration of its potential influence on survey results. This paper reviews the theory and emerging evidence of mode effects in the survey methodology and SP literatures, summarizes the findings, and points out implications for Internet SP practice and research. The SP studies that compare Internet with other modes do generally not find substantial difference. The majority of welfare estimates are equal; or somewhat lower for the Internet surveys. Further, there is no clear evidence of substantially lower quality or validity of Internet responses. However, the degree of experimental control is often low in comparative studies across survey modes, and they often confound measurement and sample composition effects. Internet offers a huge potential for experimentation and innovation in SP research, but when used to derive reliable welfare estimates for policy assessment, issues like representation and nonresponse bias for different Internet panels should receive more attention. {\textcopyright} 2011 H. Lindhjem and S. Navrud.},
author = {Lindhjem, Henrik and Navrud, St{\aa}le},
doi = {10.1561/101.00000045},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindhjem, Navrud - 2011 - Using internet in stated preference surveys A review and comparison of survey modes.pdf:pdf},
issn = {19321465},
journal = {International Review of Environmental and Resource Economics},
keywords = {Contingent valuation,Internet,Stated preferences,Survey mode},
number = {4},
pages = {309--351},
title = {{Using internet in stated preference surveys: A review and comparison of survey modes}},
volume = {5},
year = {2011}
}
@article{Grow2020,
author = {Grow, Andr{\'{e}} and Perrotta, Daniela and {Del Fava}, Emanuele and Cimentada, Jorge and Rampazzo, Francesco and Gil-Clavel, Sofia and Zagheni, Emilio},
doi = {10.31235/osf.io/ez9pb},
file = {:home/nandan/Downloads/_papers/Addressing Public Health Emergencies via Facebook Surveys.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
title = {{Addressing Public Health Emergencies via Facebook Surveys: Advantages, Challenges, and Practical Considerations}},
year = {2020}
}
@article{Battaglia2009,
abstract = {One can use raking to improve the relation between the sample and the population by adjusting the sampling weights of the cases in the sample so that the marginal totals of the adjusted weights on specified characteristics agree with the corresponding totals for the population. The raking procedure is described, and convergence issues and problems are discussed. The},
author = {Battaglia, Michael P. and Hoaglin, David C. and Frankel, Martin R.},
doi = {10.29115/sp-2009-0019},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2953-practical-considerations-in-raking-survey-data.pdf:pdf},
issn = {2168-0094},
journal = {Survey Practice},
number = {5},
pages = {1--10},
title = {{Practical considerations in raking survey data}},
volume = {2},
year = {2009}
}
@article{Neyman1934,
abstract = {Describes the mathod of random sampling and the method of purposive.},
author = {Neyman, Jerzy},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2342192.pdf:pdf},
issn = {0952-8385},
journal = {Journal of the Royal Statistical Society},
number = {4},
pages = {558--625},
title = {On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection},
volume = {97},
year = {1934}
}
@article{keeter2017low,
  title={What low response rates mean for telephone surveys},
  author={Keeter, Scott and Hatley, Nick and Kennedy, Courtney and Lau, Arnold},
  journal={Pew Research Center},
  volume={15},
  number={1},
  pages={1--39},
  year={2017}
}
@article{Mellon2017,
abstract = {A growing social science literature has used Twitter and Facebook to study political and social phenomena including for election forecasting and tracking political conversations. This research note uses a nationally representative probability sample of the British population to examine how Twitter and Facebook users differ from the general population in terms of demographics, political attitudes and political behaviour. We find that Twitter and Facebook users differ substantially from the general population on many politically relevant dimensions including vote choice, turnout, age, gender, and education. On average social media users are younger and better educated than non-users, and they are more liberal and pay more attention to politics. Despite paying more attention to politics, social media users are less likely to vote than non-users, but they are more likely to support the left leaning Labour Party when they do vote. However, we show that these apparent differences mostly arise due to the demographic composition of social media users. After controlling for age, gender, and education, no statistically significant differences arise between social media users and non-users on political attention, values or political behaviour.},
author = {Mellon, Jonathan and Prosser, Christopher},
doi = {10.1177/2053168017720008},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mellon, Prosser - 2017 - Twitter and Facebook are not representative of the general population Political attitudes and demographics of b.pdf:pdf},
issn = {20531680},
journal = {Research and Politics},
keywords = {British election study,Election forecasting,Facebook,Representativeness,Social media,Twitter},
number = {3},
pages = {1--9},
title = {{Twitter and Facebook are not representative of the general population: Political attitudes and demographics of british social media users}},
volume = {4},
year = {2017}
}
@book{Chambliss2019,
author = {Chambliss, Daniel F. and Schutt, Russel K.},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chambliss, Schutt - 2019 - Making Sense of the Social World Methods of Investigation.pdf:pdf},
title = {{Making Sense of the Social World: Methods of Investigation}},
year = {2019}
}
@article{Toepoel2020,
abstract = {Online surveys are increasingly completed on smartphones. There are several ways to structure online surveys so as to create an optimal experience for any screen size. For example, communicating through applications (apps) such as WhatsApp and Snapchat closely resembles natural turn-by-turn conversations between individuals. Web surveys currently mimic the design of paper questionnaires mostly, leading to a survey experience that may not be optimal when completed on smartphones. In this paper, we compare a research messenger design, which mimics a messenger app type of communication, to a responsive survey design. We investigate whether response quality is similar between the two designs and whether respondents' satisfaction with the survey is higher for either version. Our results show no differences for primacy effects, number of nonsubstantive answers, and dropout rate. The length of open-ended answers was shorter for the research messenger survey compared to the responsive design, and the overall time of completion was longer in the research messenger survey. The evaluation at the end of the survey showed no clear indication that respondents liked the research messenger survey more than the responsive design. Future research should focus on how to optimally design online mixed-device surveys in order to increase respondent satisfaction and data quality.},
author = {Toepoel, Vera and Lugtig, Peter and Struminskaya, Bella and Elevelt, Anne and Haan, Marieke},
doi = {10.29115/sp-2020-0010},
file = {:home/nandan/Downloads/_papers/14188-adapting-surveys-to-the-modern-world-comparing-a-research-messenger-design-to-a-regular-responsive-design-for-online-surveys.pdf:pdf},
journal = {Survey Practice},
keywords = {10,29115,data quality,doi,https,mixed-device survey,mobile survey,org,research messenger,respondent burden,sp-2020-0010},
number = {1},
pages = {1--10},
title = {{Adapting surveys to the modern world: Comparing a research messenger design to a regular responsive design for online surveys}},
volume = {13},
year = {2020}
}
@article{Tourangeau2013,
abstract = {The development and widespread use of Web surveys have resulted in an outpouring of research on their design. In this volume, Tourangeau, Conrad, and Couper provide a comprehensive summary and synthesis of the literature on this increasingly popular method of data collection. The book includes new integration of the authors' work with other important research on Web surveys, including a meta-analysis of studies that compare reports on sensitive topics in Web surveys with reports collected in other modes of data collection. Adopting the total survey error framework, the book examines sampling and coverage issues, nonresponse, measurement, and the issues involved in combining modes. In addition, the concluding chapter provides a model for understanding the errors in estimates that combine data collected in more than one mode. Web surveys have several important characteristics that affect their ability to collect accurate survey data. Discussing these in detail, the authors address basic design decisions from input widgets to background colors. They additionally focus on the visual character of Web surveys, on their ability to automatically interact with respondents, and on the Web as a method of self-administration. The Science of Web Surveys is relevant for those with the practical goal of improving their own surveys and those with an interest in understanding an increasingly important method of data collection.},
author = {Tourangeau, Roger and Conrad, Frederick G. and Couper, Mick P.},
doi = {10.1093/acprof:oso/9780199747047.001.0001},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tourangeau, Conrad, Couper - 2013 - The Science of Web Surveys.pdf:pdf},
journal = {The Science of Web Surveys},
title = {{The Science of Web Surveys}},
year = {2013}
}
@book{Polling,
abstract = {polls - n{\~{a}}o interessa},
author = {Polling, Political and Salt, The and Tribune, Lake},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Polling, Salt, Tribune - Unknown - Political Polling in the Digital Age.pdf:pdf},
isbn = {9780807138298},
title = {{Political Polling in the Digital Age}}
}
@article{Gelman1997,
author = {Gelman, Andrew and Little, Thomas C.},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/poststrat3.pdf:pdf},
keywords = {bayesian inference,election forecasting,nonresponse,opinion polls,sample sur-},
title = {Poststratification Into Many Categories Using Hierarchical Logistic Regression},
year = {1997},
journal = {Survey Methodology},
volume = {23},
pages = {127-135}
}
@article{Krueger2014,
abstract = {Given the theoretical promise of these auxiliary data for overcoming the challenge of nonresponse, survey researchers have shown an increased interest in collecting paradata, data from screening interviews, and other contextual information. However, very few studies have systematically assessed the use of these data for post-survey nonresponse adjustments. Those studies that do exist generally do not identify auxiliary variables that correlate with response propensity and key survey variables, a necessary condition for auxiliary data to be effective tools for reducing nonresponse bias. Using the National Survey of Family Growth (NSFG), this paper leverages a large set of auxiliary variables available for the full NSFG sample to assess their potential as independent tools for post-survey nonresponse adjustments. We begin by using this auxiliary information to predict response propensity (RP) for each person in the full sample. We then display descriptive estimates for a variety of attitudes and behaviors measured in the NSFG, using post-stratification weighting adjustments as well as RP adjustments followed by post-stratification. The results show that accounting for RP in the weighting adjustment often produces noteworthy differences in the estimates, thus supporting the collection of these types of auxiliary variables in practice. These results also suggest that standard post-stratification adjustments may not be entirely effective at removing nonresponse bias from all survey estimates, and that some subgroup analyses may be especially subject to bias when adjusting survey estimates using post-stratification alone.},
author = {Krueger, B. S. and West, B. T.},
doi = {10.1093/poq/nfu040},
file = {:home/nandan/Downloads/_papers/nfu040.pdf:pdf},
issn = {0033-362X},
journal = {Public Opinion Quarterly},
number = {4},
pages = {795--831},
title = {{Assessing the Potential of Paradata and Other Auxiliary Data for Nonresponse Adjustments}},
volume = {78},
year = {2014}
}
@article{VanDenBrakel2017,
abstract = {In this paper the question is addressed how alternative data sources, such as administrative and social media data, can be used in the production of official statistics. Since most surveys at national statistical institutes are conducted repeatedly over time, a multivariate structural time series modelling approach is proposed to model the series observed by a repeated surveys with related series obtained from such alternative data sources. Generally, this improves the precision of the direct survey estimates by using sample information observed in preceding periods and information from related auxiliary series. This model also makes it possible to utilize the higher frequency of the social media to produce more precise estimates for the sample survey in real time at the moment that statistics for the social media become available but the sample data are not yet available. The concept of cointegration is applied to address the question to which extent the alternative series represent the same phenomena as the series observed with the repeated survey. The methodology is applied to the Dutch Consumer Confidence Survey and a sentiment index derived from social media.},
author = {{Van Den Brakel}, Jan and S{\"{o}}hler, Emily and Daas, Piet and Buelens, Bart},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Den Brakel et al. - 2017 - Social media as a data source for official statistics the dutch consumer confidence index.pdf:pdf},
issn = {14920921},
journal = {Survey Methodology},
keywords = {Big data,Cointegration,Design-based inference,Model-based inference,Nowcasting,Structural time series modelling},
number = {2},
pages = {183--210},
title = {{Social media as a data source for official statistics; the dutch consumer confidence index}},
volume = {43},
year = {2017}
}
@article{DeVellis2016,
abstract = {Scale Development: Theory and Applications has helped students, practitioners, and researchers design and develop scales. In this updated version, DeVellis has expanded his coverage of factor analysis and has added a new chapter on item response theory.},
author = {DeVellis, Robert F},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeVellis - 2016 - Scale Development Theory and Applications ( Fourth Edition Robert).pdf:pdf},
journal = {SAGE Publication},
pages = {256},
title = {{Scale Development Theory and Applications ( Fourth Edition Robert)}},
url = {https://b-ok.cc},
volume = {4},
year = {2016}
}
@article{Czajka2016,
abstract = {Key Findings: *  A long-term decline in survey response rates has accelerated in recent years. *  The increase in nonresponse has been much greater in telephone surveys than in face-to-face surveys. *  Response rates are a poor predictor of nonresponse bias, which tends to be item-specific. *  Payment of incentives is the most effective strategy to increase response rates. This report reviews recent assessments of trends in response rates among primarily federal surveys and the reasons for declining response rates; documents response trends in seven major Health and Human Services surveys and the Census Bureau's Current Population Survey from the mid-1990s through the most recent year available; examines what is known about the relationship between response rates and nonresponse bias; reviews approaches to addressing nonresponse and its effects; and summarizes key conclusions.},
author = {Czajka, John L. and Beyler, Amy},
file = {:home/nandan/Downloads/_papers/Decliningresponserates.pdf:pdf},
journal = {Mathematica Policy Research},
keywords = {cross-sectional s,nonresponse bias,survey burden},
number = {202},
pages = {1--54},
title = {{Declining Response Rates in Federal Surveys: Trends and Implications}},
url = {https://www.mathematica-mpr.com/our-publications-and-findings/publications/declining-response-rates-in-federal-surveys-trends-and-implications-background-paper},
volume = {I},
year = {2016}
}
@article{Perrotta2020,
abstract = {In the absence of medical treatment and vaccination, the mitigation and containment of the ongoing COVID-19 pandemic relies on behavioral changes. Timely data on attitudes and behaviors are thus necessary to develop optimal intervention strategies and to assess the consequences of the pandemic for different demographic groups. We developed a rapid response monitoring system via a continuously run online survey (the "COVID-19 Health Behavior Survey") across eight countries (Belgium, France, Germany, Italy, the Netherlands, Spain, the United Kingdom, the United States). The survey was specifically designed to collect key information on people&#039;s health status, behaviors, close social contacts, and attitudes in response to the COVID-19 pandemic. We developed an innovative approach to recruit participants via targeted Facebook advertisement campaigns in order to generate balanced samples for post-stratification. In this paper, we present results for the period from March 13-April 19, 2020. We estimate important differences by sex: women show a substantially higher perception of threat along with a lower level of confidence in the health system. This is paralleled by sex-specific behaviors, with women more likely to adopt a wide range of preventive behaviors. We thus expect behavior to serve as a protective factor for women. Our findings also show a higher level of awareness and concern among older respondents, in line with the evidence that the elderly are at highest risk of severe complications following infection from COVID-19. While across all the samples respondents were less concerned for themselves than for their country or for the world, we also observed substantial temporal and spatial heterogeneity in terms of confidence in institutions and responses to non-pharmaceutical interventions.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis study was funded through the support of the Max Planck Institute for Demographic Research, which is part of the Max Planck Society.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesDue to data protection regulations, the data are not publicly available. For queries about the data, please contact the corresponding authors.},
author = {Perrotta, Daniela and Grow, Andr{\'{e}} and Rampazzo, Francesco and Cimentada, Jorge and {Del Fava}, Emanuele and Gil-Clavel, Sofia and Zagheni, Emilio},
doi = {10.1101/2020.05.09.20096388},
file = {:home/nandan/Downloads/_papers/2020.05.09.20096388v2.full.pdf:pdf},
pages = {1--17},
title = {{Behaviors and attitudes in response to the COVID-19 pandemic: Insights from a cross-national Facebook survey}},
journal = {EPJ Data Science},
volume = {10},
issue = {1},
year = {2020}
}
@article{Tourangeau2010,
abstract = {Survey researchers have long speculated that there may be a link between nonresponse and measurement error - that is, people likely to become nonrespondents to a survey are also likely to make poor reporters if they do take part. Still, there is surprisingly little evidence of such a link. It could be that nonresponse is generally the product of one set of factors and reporting errors, the product of an unrelated set, or both nonresponse and reporting errors may be item-specific so that no general relationship between the two is likely to emerge. Our study examined a situation in which we thought there would be a link between response propensities and the propensity to give inaccurate answers. We asked samples of voters and nonvoters to take part in a survey that included items about voting. Past research shows that nonvoters misreport that fact and that they are less likely than voters in general to take part in surveys. We thought we could heighten the differences between voters and nonvoters in both response rates and levels of misreporting if we characterized the survey as being about politics. However, only nonresponse biases were larger when the topic of the survey was described as political, and this difference was only marginally significant. These two ways of framing the study had even smaller effects on estimates derived from other items in the questionnaire. The overall biases in estimates derived from the voting items are very substantial, and both nonresponse and measurement error contribute to them. {\textcopyright} The Author 2010. Published by Oxford University Press on behalf of the American Association for Public Opinion Research. All rights reserved.},
author = {Tourangeau, Roger and Groves, Robert M. and Redline, Cleo D.},
doi = {10.1093/poq/nfq004},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tourangeau, Groves, Redline - 2010 - Sensitive topics and reluctant respondents Demonstrating a link between nonresponse bias and measur.pdf:pdf},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {3},
pages = {413--432},
title = {{Sensitive topics and reluctant respondents: Demonstrating a link between nonresponse bias and measurement error}},
volume = {74},
year = {2010}
}

@article{Groves2010a,
abstract = {"Total survey error" is a conceptual framework describing statistical error properties of sample survey statistics. Early in the history of sample surveys, it arose as a tool to focus on implications of various gaps between the conditions under which probability samples yielded unbiased estimates of finite population parameters and practical situations in implementing survey design. While the framework permits design-based estimates of various error components, many of the design burdens to produce those estimates are large, and in practice most surveys do not implement them. Further, the framework does not incorporate other, nonstatistical, dimensions of quality that are commonly utilized in evaluating statistical information. The importation of new modeling tools brings new promise to measuring total survey error components, but also new challenges. A lasting value of the total survey error framework is at the design stage of a survey, to attempt a balance of costs and various errors. Indeed, this framework is the central organizing structure of the field of survey methodology. {\textcopyright} The Author 2011.},
author = {Groves, Robert M. and Lyberg, Lars},
doi = {10.1093/poq/nfq065},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/nfq065.pdf:pdf},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {5},
pages = {849--879},
title = {{Total survey error: Past, present, and future}},
volume = {74},
year = {2010}
}
@article{Shirani-Mehr2018,
abstract = {It is well known among researchers and practitioners that election polls suffer from a variety of sampling and nonsampling errors, often collectively referred to as total survey error. Reported margins of error typically only capture sampling variability, and in particular, generally ignore nonsampling errors in defining the target population (e.g., errors due to uncertainty in who will vote). Here, we empirically analyze 4221 polls for 608 state-level presidential, senatorial, and gubernatorial elections between 1998 and 2014, all of which were conducted during the final three weeks of the campaigns. Comparing to the actual election outcomes, we find that average survey error as measured by root mean square error is approximately 3.5 percentage points, about twice as large as that implied by most reported margins of error. We decompose survey error into election-level bias and variance terms. We find that average absolute election-level bias is about 2 percentage points, indicating that polls for a given election often share a common component of error. This shared error may stem from the fact that polling organizations often face similar difficulties in reaching various subgroups of the population, and that they rely on similar screening rules when estimating who will vote. We also find that average election-level variance is higher than implied by simple random sampling, in part because polling organizations often use complex sampling designs and adjustment procedures. We conclude by discussing how these results help explain polling failures in the 2016 U.S. presidential election, and offer recommendations to improve polling practice.},
author = {Shirani-Mehr, Houshmand and Rothschild, David and Goel, Sharad and Gelman, Andrew},
doi = {10.1080/01621459.2018.1448823},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Disentangling Bias and Variance in Election Polls.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Margin of error,Nonsampling error,Polling bias,Total survey error},
number = {522},
pages = {607--614},
title = {{Disentangling Bias and Variance in Election Polls}},
url = {https://doi.org/10.1080/01621459.2018.1448823},
volume = {113},
year = {2018}
}
@article{Park2004,
abstract = {We fit a multilevel logistic regression model for the mean of a binary response variable conditional on poststratification cells. This approach combines the modeling approach often used in small-area estimation with the population information used in poststratification (see Gelman and Little 1997, Survey Methodology 23:127-135). To validate the method, we apply it to U.S. preelection polls for 1988 and 1992, poststratified by state, region, and the usual demographic variables. We evaluate the model by comparing it to state-level election outcomes. The multilevel model outperforms more commonly used models in political science. We envision the most important usage of this method to be not forecasting elections but estimating public opinion on a variety of issues at the state level. {\textcopyright} Society for Political Methodology 2004; all rights reserved.},
author = {Park, David K. and Gelman, Andrew and Bafumi, Joseph},
doi = {10.1093/pan/mph024},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Gelman, Bafumi - 2004 - Bayesian multilevel estimation with poststratification State-level estimates from national polls.pdf:pdf},
issn = {10471987},
journal = {Political Analysis},
number = {4},
pages = {375--385},
title = {{Bayesian multilevel estimation with poststratification: State-level estimates from national polls}},
volume = {12},
year = {2004}
}
@article{Biemer2010,
abstract = {The total survey error (TSE) paradigm provides a theoretical framework for optimizing surveys by maximizing data quality within budgetary constraints. In this article, the TSE paradigm is viewed as part of a much larger design strategy that seeks to optimize surveys by maximizing total survey quality; i.e., quality more broadly defined to include user-specified dimensions of quality. Survey methodology, viewed within this larger framework, alters our perspectives on the survey design, implementation, and evaluation. As an example, although a major objective of survey design is to maximize accuracy subject to costs and timeliness constraints, the survey budget must also accommodate additional objectives related to relevance, accessibility, interpretability, comparability, coherence, and completeness that are critical to a survey's "fitness for use." The article considers how the total survey quality approach can be extended beyond survey design to include survey implementation and evaluation. In doing so, the "fitness for use" perspective is shown to influence decisions regarding how to reduce survey error during design implementation and what sources of error should be evaluated in order to assess the survey quality today and to prepare for the surveys of the future. {\textcopyright} The Author 2011.},
author = {Biemer, Paul P.},
doi = {10.1093/poq/nfq058},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/nfq058.pdf:pdf},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {5},
pages = {817--848},
title = {{Total survey error: Design, implementation, and evaluation}},
volume = {74},
year = {2010}
}
@article{Wang2015,
abstract = {Election forecasts have traditionally been based on representative polls, in which randomly sampled individuals are asked who they intend to vote for. While representative polling has historically proven to be quite effective, it comes at considerable costs of time and money. Moreover, as response rates have declined over the past several decades, the statistical benefits of representative sampling have diminished. In this paper, we show that, with proper statistical adjustment, non-representative polls can be used to generate accurate election forecasts, and that this can often be achieved faster and at a lesser expense than traditional survey methods. We demonstrate this approach by creating forecasts from a novel and highly non-representative survey dataset: a series of daily voter intention polls for the 2012 presidential election conducted on the Xbox gaming platform. After adjusting the Xbox responses via multilevel regression and poststratification, we obtain estimates which are in line with the forecasts from leading poll analysts, which were based on aggregating hundreds of traditional polls conducted during the election cycle. We conclude by arguing that non-representative polling shows promise not only for election forecasting, but also for measuring public opinion on a broad range of social, economic and cultural issues.},
author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
doi = {10.1016/j.ijforecast.2014.06.001},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2015 - Forecasting elections with non-representative polls.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Election forecasting,Multilevel regression and poststratification,Non-representative polling},
month = {jul},
number = {3},
pages = {980--991},
publisher = {Elsevier B.V.},
title = {{Forecasting elections with non-representative polls}},
volume = {31},
year = {2015}
}
@article{Couper2017,
abstract = {This review focuses on recent methodological and technological developments in survey data collection. Surveys are facing unprecedented challenges from both societal and technological changes. Against this backdrop, I review the survey profession's response to these challenges and developments to enhance and extend the survey tool. I discuss the decline in random digit dialing and the rise of address-based sampling, along with the corresponding shift from telephone surveys to self-administered (mail and or Web) modes. I discuss the rise in nonprobability sampling approaches, especially those associated with online data collection. I also review so-called big data alternatives to surveys. Finally, I discuss a number of recent methodological and technological trends designed to modernize the survey method. I conclude that although they face a number of major challenges, surveys remain a robust and flexible method for collecting data on, and making inference to, populations.},
author = {Couper, Mick P.},
doi = {10.1146/annurev-soc-060116-053613},
file = {:home/nandan/Downloads/_papers/annurev-soc-060116-053613.pdf:pdf},
issn = {03600572},
journal = {Annual Review of Sociology},
keywords = {Adaptive design,Address-based sampling,Mail surveys,Mixed-mode data collection,Nonprobability methods,Random digit dialing,Responsive design,Survey mode,Telephone surveys,Web surveys},
pages = {121--145},
title = {{New developments in survey data collection}},
volume = {43},
year = {2017}
}
@article{Dutwin2017,
abstract = {Nonprobability samples have gained mass popularity and use in many research circles, including market research and some political research. One justification for the use of nonprobability samples is that low response rate probability surveys have nothing significant to offer over and above a "well built" nonprobability sample. Utilizing an elemental approach, we compare a range of samples, weighting, and modeling procedures in an analysis that evaluates the estimated bias of various cross-tabulations of core demographics. Specifically, we compare a battery of bias related metrics for nonprobability panels, dualframe telephone samples, and a high-quality in-person sample. Results indicate that there is roughly a linear trend, with nonprobability samples attaining the greatest estimated bias, and the in-person sample, the lowest. Results also indicate that the bias estimates vary widely for the nonprobability samples compared to either the telephone or in-person samples, which themselves tend to have consistently smaller amounts of estimated bias. Specifically, both weighted and unweighted dual-frame telephone samples were found to have about half the estimated bias compared to analogous nonprobability samples. Advanced techniques such as propensity weighting and sample matching did not improve these measures, and in some cases made matters worse. Implications for "fit for purpose" in survey research are discussed given these findings.},
author = {Dutwin, David and Buskirk, Trent D.},
doi = {10.1093/poq/nfw061},
file = {:home/nandan/Downloads/_papers/oup-accepted-manuscript-2017.pdf:pdf},
issn = {15375331},
journal = {Public Opinion Quarterly},
number = {January},
pages = {213--249},
title = {{Apples to Oranges or Gala versus Golden Delicious?}},
volume = {81},
year = {2017}
}
@article{Schneider2019,
abstract = {In this article, we explore the use of Facebook targeted advertisements for the collection of survey data. We illustrate the potential of survey sampling and recruitment on Facebook through the example of building a large employee–employer linked data set as part of The Shift Project. We describe the workflow process of targeting, creating, and purchasing survey recruitment advertisements on Facebook. We address concerns about sample selectivity and apply poststratification weighting techniques to adjust for differences between our sample and that of “gold standard” data sources. We then compare univariate and multivariate relationships in the Shift data against the Current Population Survey and the National Longitudinal Survey of Youth 1997. Finally, we provide an example of the utility of the firm-level nature of the data by showing how firm-level gender composition is related to wages. We conclude by discussing some important remaining limitations of the Facebook approach, as well as highlighting some unique strengths of the Facebook targeted advertisement approach, including the ability for rapid data collection in response to research opportunities, rich and flexible sample targeting capabilities, and low cost, and we suggest broader applications of this technique.},
author = {Schneider, Daniel and Harknett, Kristen},
doi = {10.1177/0049124119882477},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schneider, Harknett - 2019 - What's to Like Facebook as a Tool for Survey Data Collection.pdf:pdf},
issn = {15528294},
journal = {Sociological Methods and Research},
keywords = {firm-level data,gender,nonprobability sampling,survey methods,work},
title = {{What's to Like? Facebook as a Tool for Survey Data Collection}},
year = {2019}
}
@article{Heffetz2019,
abstract = {How high is unemployment? How low is labor force participation? Is obesity more prevalent among men? How large are household expenditures? We study the sources of the relevant official statistics-The Current Population Survey, the Behavioral Risk Factor Surveillance System, and the Consumer Expenditure Survey-And find that the answers depend on whether we look at easy-or at difficult-To-reach respondents, measured by the number of call and visit attempts made by interviewers. A challenge to the (conditionally-)random-nonresponse assumption, these findings empirically substantiate the theoretical warning against making population-wide estimates from surveys with low response rates.},
author = {Heffetz, Ori and Reeves, Daniel B.},
doi = {10.1162/rest_a_00748},
file = {:home/nandan/Downloads/_papers/rest_a_00748.pdf:pdf},
issn = {15309142},
journal = {Review of Economics and Statistics},
number = {1},
pages = {176--191},
title = {{Difficulty of Reaching Respondents and Nonresponse Bias: Evidence from Large Government Surveys}},
volume = {101},
year = {2019}
}
@article{deville1992calibration,
  title={Calibration estimators in survey sampling},
  author={Deville, Jean-Claude and S{\"a}rndal, Carl-Erik},
  journal={Journal of the American Statistical Association},
  volume={87},
  number={418},
  pages={376--382},
  year={1992},
  publisher={Taylor \& Francis}
}
@article{Cooper1964,
author = {Cooper, Sanford L},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/cooper1964.pdf:pdf},
journal = {Journal of Marketing Research},
number = {4},
pages = {45--48},
title = {{Random sampling by telephone - An improved method}},
volume = {1},
year = {1964}
}
@article{sudman1973uses,
  title={The uses of telephone directories for survey sampling},
  author={Sudman, Seymour},
  journal={Journal of Marketing Research},
  volume={10},
  number={2},
  pages={204--207},
  year={1973},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@article{Glasser1972,
author = {Glasser, Gerald J and Metzger, Gale D},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/glasser1972.pdf:pdf},
journal = {Journal of Marketing Research},
number = {1},
pages = {59--64},
title = {Random-Digit Dialing as a Method of Telephone Sampling},
volume = {9},
year = {1972}
}
@article{Mercer2018,
author = {Mercer, Andrew and Lau, Arnold and Kennedy, Courtney},
file = {:home/nandan/Downloads/_papers/Weighting-Online-Opt-In-Samples.pdf:pdf},
journal = {Pew Research Center},
title = {For Weighting Online Opt-In Samples, What Matters Most?},
year = {2018}
}
@incollection{Groves2010,
author = {Groves, Robert M. and Singer, Eleanor and Lepkowski, James M. and Heeringa, Steven G. and Alwin, Duane F.},
booktitle = {A Telescope on Society: Survey Research and Social Science at the University of Michigan and Beyond},
doi = {10.4324/9780429314254-2},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Groves et al. - 2010 - Survey methodology.pdf:pdf},
isbn = {0472098489},
issn = {0162-1459},
pages = {21--64},
title = {{Survey methodology}},
publisher = {University of Michigan Press},
year = {2010}
}
@article{Kolenikov2016,
author = {Kolenikov, Stas J.},
doi = {10.29115/SP-2016-0014},
file = {:home/nandan/Downloads/_papers/2809-post-stratification-or-non-response-adjustment.pdf:pdf},
issn = {2168-0094},
journal = {Survey Practice},
keywords = {10,29115,calibration,doi,https,non-response adjustment,org,raking,sp-2016-0014,weighting},
month = {aug},
number = {3},
pages = {1--12},
title = {{Post-stratification or a non-response adjustment?}},
volume = {9},
year = {2016}
}
@book{Fowler2014,
abstract = {The Fourth Edition of the bestselling Survey Research Methods presents the very latest methodological knowledge on surveys. Author Floyd J. Fowler Jr. provides students and researchers who want to collect, analyze, or read about survey data with a sound basis for evaluating how each aspect of a survey can affect its precision, accuracy, and credibility. The Fourth Edition has been updated in four primary ways: it much more prominently addresses the growth of the Internet for data collection and the subsequent rapid expansion of online survey usage; it addresses the precipitous drop in response rates for telephone surveys, particularly those based on random-digit dialing; it offers new and expanded coverage monitoring the continued improvement in techniques for presurvey evaluation of questions; and it addresses the growing role of individual cell phone in addition - and often instead of - household landlines. Two new chapters, “The Nature of Error in Surveys” and “Issues in Analyzing Survey Data,” further emphasize the importance of minimizing nonsampling errors through superior question design, quality interviewing, and high response rates. Key FeaturesCovers the expansion of cell phone use and legislation regarding them; this offers survey researchers guidance as to policy implications and practical application Expands the coverage of web-based and online surveys as well as the latest resources available to the beginning and expert researcher Offers in-depth discussion of non-response and sample size issues, especially as they relate to powerFocuses on data analysis, especially with regard to bivariate and multivariate approaches. Fowler walks students and researchers through the various types of analyses one would do once the data are ready to analyze. Provides a list of strengths and weaknesses for each of the different types of survey data collection, including the more recent web-based approachesIncludes updated references and survey examples that offer various levels of students and researchers other exemplary literature and modelsSurvey Research Methods, Fourth Edition gives compact, yet comprehensive coverage, making it an ideal companion or beginning text. Praise for Floyd J. Fowler, Jr. and the previous edition:“Fowler is smart, straightforward, and sensible in writing about research methodology. Students have a lot to gain from his wisdom and experience.” —Mark Berends, Vanderbilt University},
author = {Fowler, Floyd J.},
booktitle = {Sage Publications, Inc},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fowler - 2014 - Survey Research Methods (5th edition).pdf:pdf},
isbn = {9781452259000},
pages = {185},
title = {{Survey Research Methods (5th edition)}},
year = {2014}
}
@article{Ambel2021,
author = {Ambel, Alemayehu and McGee, Kevin and Tsegay, Asmelash},
file = {:home/nandan/Downloads/_papers/Reducing-Bias-in-Phone-Survey-Samples-Effectiveness-of-Reweighting-Techniques-Using-Face-to-Face-Surveys-as-Frames-in-Four-African-Countries.pdf:pdf},
number = {May},
title = {{Reducing Bias in Phone Survey Samples Effectiveness of Reweighting Techniques Using Face-to-Face Surveys as Frames in Four African Countries}},
year = {2021}
}

@article{peer2017beyond,
  title={{Beyond the Turk: Alternative platforms for crowdsourcing behavioral research}},
  author={Peer, Eyal and Brandimarte, Laura and Samat, Sonam and Acquisti, Alessandro},
  journal={Journal of Experimental Social Psychology},
  volume={70},
  pages={153--163},
  year={2017},
  publisher={Elsevier}
}

@article{stewart2015average,
  title={{The average laboratory samples a population of 7,300 Amazon Mechanical Turk workers}},
  author={Stewart, Neil and Ungemach, Christoph and Harris, Adam JL and Bartels, Daniel M and Newell, Ben R and Paolacci, Gabriele and Chandler, Jesse},
  journal={Judgment and Decision Making},
  volume={10},
  number={5},
  pages={479--491},
  year={2015},
  publisher={Cambridge University Press}
}

@article{krefeld2024exposing,
  title={Exposing omitted moderators: Explaining why effect sizes differ in the social sciences},
  author={Krefeld-Schwalb, Antonia and Sugerman, Eli Rosen and Johnson, Eric J},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={12},
  pages={e2306281121},
  year={2024},
  publisher={National Acad Sciences}
}


@article{aridor2024experiments,
  title={Experiments on social media},
  author={Aridor, Guy and Jim{\'e}nez Dur{\'a}n, Rafael and Levy, Ro’ee and Song, Lena},
  journal = {CEPR Discussion Paper No. 19861},
  year={2025},
  publisher={CEPR Press}
}

@article{yeager2011comparing,
  title={{Comparing the accuracy of RDD telephone surveys and internet surveys conducted with probability and non-probability samples}},
  author={Yeager, David S and Krosnick, Jon A and Chang, LinChiat and Javitz, Harold S and Levendusky, Matthew S and Simpser, Alberto and Wang, Rui},
  journal={Public Opinion Quarterly},
  volume={75},
  number={4},
  pages={709--747},
  year={2011},
  publisher={Oxford University Press}
}

@article{dever2008internet,
  title={Internet surveys: Can statistical adjustments eliminate coverage bias?},
  author={Dever, Jill A and Rafferty, Ann and Valliant, Richard},
  journal={Survey Research Methods},
  volume={2},
  number={2},
  pages={47--60},
  year={2008}
}

@article{donati2024can,
  title={{Can Facebook ads prevent malaria? Two field experiments in India}},
  journal={World Bank Policy Research Working Paper Series},
  author={Donati, Dante and Rao, Nandan and Olvera, Victor Hugo Orozco and Munoz-Boudet, Ana Maria},
  year={2024},
  publisher={The World Bank},
  number={10967}
}


@article{samuels2013using,
  title={Using {Facebook} as a subject recruitment tool for survey-experimental research},
  author={Samuels, David J and Zucco, Cesar},
  journal={Available at SSRN 2101458},
  year={2013}
}


@article{allcott2020welfare,
  title={The welfare effects of social media},
  author={Allcott, Hunt and Braghieri, Luca and Eichmeyer, Sarah and Gentzkow, Matthew},
  journal={American Economic Review},
  volume={110},
  number={3},
  pages={629--676},
  year={2020},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203}
}


@article{sances2018ideology,
  title={{Ideology and vote choice in US mayoral elections: Evidence from Facebook surveys}},
  author={Sances, Michael W},
  journal={Political Behavior},
  volume={40},
  pages={737--762},
  year={2018},
  publisher={Springer}
}


@article{beam2023social,
  title={Social media as a recruitment and data collection tool: Experimental evidence on the relative effectiveness of web surveys and chatbots},
  author={Beam, Emily A},
  journal={Journal of Development Economics},
  volume={162},
  pages={103069},
  year={2023},
  publisher={Elsevier}
}

@article{boas2020recruiting,
  title={{Recruiting large online samples in the United States and India: Facebook, Mechanical Turk, and Qualtrics}},
  author={Boas, Taylor C and Christenson, Dino P and Glick, David M},
  journal={Political Science Research and Methods},
  volume={8},
  number={2},
  pages={232--250},
  year={2020},
  publisher={Cambridge University Press}
}


@article{neundorf2023improve,
  title={How to improve representativeness and cost-effectiveness in samples recruited through {Meta}: A comparison of advertisement tools},
  author={Neundorf, Anja and {\"O}zt{\"u}rk, Aykut},
  journal={PLOS ONE},
  volume={18},
  number={2},
  pages={e0281243},
  year={2023},
  publisher={Public Library of Science San Francisco, CA USA}
}


@article{zhang2020quota,
  title={{Quota sampling using Facebook advertisements}},
  author={Zhang, Baobao and Mildenberger, Matto and Howe, Peter D and Marlon, Jennifer and Rosenthal, Seth A and Leiserowitz, Anthony},
  journal={Political Science Research and Methods},
  volume={8},
  number={3},
  pages={558--564},
  year={2020},
  publisher={Cambridge University Press}
}


@article{rosenzweig2020survey,
  title={Survey sampling in the Global South using {Facebook} advertisements},
  author={Rosenzweig, Leah and Bergquist, Parrish and Pham, Katherine Hoffmann and Rampazzo, Francesco and Mildenberger, Matto},
  journal={Political Science Research and Methods},
  year={2025},
  doi = {10.1017/psrm.2025.18}
}


@article{rand2014social,
  title={Social heuristics shape intuitive cooperation},
  author={Rand, David G and Peysakhovich, Alexander and Kraft-Todd, Gordon T and Newman, George E and Wurzbacher, Owen and Nowak, Martin A and Greene, Joshua D},
  journal={Nature Communications},
  volume={5},
  number={1},
  pages={3677},
  year={2014},
  publisher={Nature Publishing Group UK London}
}


@article{donati2022marketing,
  title={Using Social Media to Change Gender Norms: An Experiment within {Facebook Messenger in India}},
  author={Donati, Dante and Orozco-Olvera, Victor and Rao, Nandan},
  year={2022},
  journal={World Bank Policy Research Working Paper Series},
  publisher={The World Bank},
  number={10199}
}

@article{evans2023outcomes,
  title={{Outcomes of a social media campaign to promote COVID-19 vaccination in Nigeria}},
  author={Evans, W Douglas and Bingenheimer, Jeffrey B and Long, Michael and Ndiaye, Khadidiatou and Donati, Dante and Rao, Nandan M and Akaba, Selinam and Nsofor, Ifeanyi and Agha, Sohail},
  journal={PLOS ONE},
  volume={18},
  number={9},
  pages={e0290757},
  year={2023},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{ichimiya2023evaluation,
  title={Evaluation of response to incentive recruitment strategies in a social media-based survey},
  author={Ichimiya, Megumi and Muller-Tabanera, Hope and Cantrell, Jennifer and Bingenheimer, Jeffrey B and Gerard, Raquel and Hair, Elizabeth C and Donati, Dante and Rao, Nandan and Evans, W Douglas},
  journal={Digital Health},
  volume={9},
  pages={20552076231178430},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}




@article{agha2021drivers,
  title={{Drivers of COVID-19 vaccine uptake amongst healthcare workers (HCWS) in Nigeria}},
  author={Agha, Sohail and Chine, Adaobi and Lalika, Mathias and Pandey, Samikshya and Seth, Aparna and Wiyeh, Alison and Seng, Alyssa and Rao, Nandan and Badshah, Akhtar},
  journal={Vaccines},
  volume={9},
  number={10},
  pages={1162},
  year={2021},
  publisher={MDPI}
}

@article{rao2020conducting,
  title={Conducting surveys and interventions entirely online: a {Virtual Lab} practitioner’s manual},
  author={Rao, Nandan and Donati, Dante and Orozco, Victor},
  journal={World Bank},
  year={2020}
}


@article{narasimhan2015marketing,
  title={Marketing science in emerging markets},
  author={Narasimhan, Laxman and Srinivasan, Kannan and Sudhir, Karunakaran},
  journal={Marketing Science},
  volume={34},
  number={4},
  pages={473--479},
  year={2015},
  publisher={INFORMS}
}


@book{rossi2013handbook,
  title={Handbook of survey research},
  author={Rossi, Peter H and Wright, James D and Anderson, Andy B},
  year={2013},
  publisher={Academic Press}
}

@book{vannette2017palgrave,
  title={The Palgrave handbook of survey research},
  author={Vannette, David L and Krosnick, Jon A},
  year={2017},
  publisher={Springer}
}

@article{hyndman2006another,
  title={Another look at measures of forecast accuracy},
  author={Hyndman, Rob J and Koehler, Anne B},
  journal={International Journal of Forecasting},
  volume={22},
  number={4},
  pages={679--688},
  year={2006},
  publisher={Elsevier}
}

@article{kalton2003weighting,
  title={Weighting methods},
  author={Kalton, Graham and Flores-Cervantes, Ismael},
  journal={Journal of Official Statistics},
  volume={19},
  number={2},
  pages={81--97},
  year={2003}
}

@article{lee2009estimation,
  title={Estimation for volunteer panel web surveys using propensity score adjustment and calibration adjustment},
  author={Lee, Sunghee and Valliant, Richard},
  journal={Sociological Methods \& Research},
  volume={37},
  number={3},
  pages={319--343},
  year={2009},
  publisher={SAGE Publications}
}

@article{brick2011future,
  title={The future of survey sampling},
  author={Brick, J Michael},
  journal={Public Opinion Quarterly},
  volume={75},
  number={5},
  pages={872--888},
  year={2011},
  publisher={Oxford University Press}
}

@article{mercer2017weighting,
  title={How different weighting methods work},
  author={Mercer, Andrew and Lau, Arnold and Kennedy, Courtney and Keeter, Scott},
  year={2017},
  journal = {Pew Research Center},
}

@article{toubia2025twin,
  title={Twin-2K-500: A Data Set for Building Digital Twins of over 2,000 People Based on Their Answers to over 500 Questions},
  author={Toubia, Olivier and Gui, George Z and Peng, Tianyi and Merlau, Daniel J and Li, Ang and Chen, Haozhe},
  journal={Marketing Science},
  year={2025},
  publisher={INFORMS}
}

@article{palan2018prolific,
  title={Prolific. ac—A subject pool for online experiments},
  author={Palan, Stefan and Schitter, Christian},
  journal={Journal of behavioral and experimental finance},
  volume={17},
  pages={22--27},
  year={2018},
  publisher={Elsevier}
}

@article{helbing2022digital,
  title={Digital Twins: Potentials, Ethical Issues, and Limitations},
  author={Helbing, Dirk and Argota S{\'a}nchez-Vaquerizo, Alex},
  journal={arXiv preprint arXiv:2208.04289},
  year={2022},
  url={https://arxiv.org/abs/2208.04289}
}

@article{lin2022human,
  title={Human Digital Twin: A Survey},
  author={Lin, Liming and Chen, Aftab and Ali, Christopher and Nugent, Ian and Cleland, Rongyang and Li, Dazhi and Gao, Hang and Wang, Yajie and Wang, Huansheng},
  journal={arXiv preprint arXiv:2212.05937},
  year={2022},
  url={https://arxiv.org/abs/2212.05937}
}

@article{fett2025practical,
  title={Practical insights into the perception of digital twins: An analysis of surveys},
  author={Fett, Tim and Mehlhose, Steffen and Nigischer, Christian and Schroeder, Christian},
  journal={Forschung im Ingenieurwesen / Engineering Research},
  volume={89},
  number={1},
  pages={1--14},
  year={2025},
  publisher={Springer},
  doi={10.1007/s10010-025-00879-y},
  url={https://link.springer.com/article/10.1007/s10010-025-00879-y}
}


@article{goel2015non,
  title        = {Non-representative surveys: Fast, cheap, and mostly accurate},
  author       = {Goel, Sharad and Obeng, Adam and Rothschild, David},
  journaltitle = {Working Paper}, 
  year         = {2015}          
  % year       = {2015}           
}



@misc{norc2024gss,
  title        = {GSS Follow-on Program Brochure},
  author       = {{NORC at the University of Chicago}},
  year         = {2024},
  note         = {Accessed January 2025},
  url          = {https://gss.norc.org/Documents/GSS%20Follow-on%20Brochure.pdf}
}

@article{Wichachai2016,
author = {Wichachai, Suparp and Songserm, Nopparat and Akakul, Theerawut and Kuasiri, Chanapong},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/APJCP_Volume 17_Issue 7_Pages 3505-3510.pdf:pdf},
issn = {15137368},
journal = {Asian Pacific Journal of Cancer Prevention},
keywords = {Cervical cancer screening,Health belief model,Social marketing theory,Thailand},
number = {7},
pages = {3505--3510},
pmid = {27510000},
title = {{Effects of application of social marketing theory and the health belief model in promoting cervical cancer screening among targeted women in Sisaket province, Thailand}},
volume = {17},
year = {2016}
}


@article{Allocation2019,
author = {Allocation, Optimal and Stratified, I N and Schemes, Sampling},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/msc_optimal_allocation.pdf:pdf},
number = {199163},
title = {{Wojciech W{\'{o}}jciak}},
year = {2019}
}
@article{American2009,
author = {American, Source and Journal, Economic and Economics, Applied and October, No},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/American Economic Journal Applied Economics Volume 1 issue 4 2009 [doi 10.2307_25760187] Miriam Bruhn and David McKenzie -- In Pursuit of Balance- Randomization in Practice in Development Field Expe.pdf:pdf},
number = {4},
pages = {200--232},
title = {{American Economic Association In Pursuit of Balance : Randomization in Practice in Development Field Experiments Author ( s ): Miriam Bruhn and David McKenzie In Pursuit of Balance : Randomization in Practice in Development Field Experiments1}},
volume = {1},
year = {2009}
}
@article{Antos2010,
abstract = {We consider the problem of actively learning the mean values of distributions associated with a finite number of options. The decision maker can select which option to generate the next observation from, the goal being to produce estimates with equally good precision for all the options. If sample means are used to estimate the unknown values then the optimal solution, assuming that the distributions are known up to a shift, is to sample from each distribution proportional to its variance. No information other than the distributions' variances is needed to calculate the optimal solution. In this paper we propose an incremental algorithm that asymptotically achieves the same loss as an optimal rule. We prove that the excess loss suffered by this algorithm, apart from logarithmic factors, scales as n- 3 / 2, which we conjecture to be the optimal rate. The performance of the algorithm is illustrated on a simple problem. Crown Copyright {\textcopyright} 2010.},
author = {Antos, Andr{\'{a}}s and Grover, Varun and Szepesv{\'{a}}ri, Csaba},
doi = {10.1016/j.tcs.2010.04.007},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1-s2.0-S0304397510002021-main.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Active learning,Heteroscedastic noise,Regression,Sequential allocation,Sequential analysis},
number = {29-30},
pages = {2712--2728},
publisher = {Elsevier B.V.},
title = {{Active learning in heteroscedastic noise}},
url = {http://dx.doi.org/10.1016/j.tcs.2010.04.007},
volume = {411},
year = {2010}
}
@unpublished{Aufenanger2017,
abstract = {This paper proposes a way of using observational pretest data for the design of experiments. In particular, this paper trains a random forest on the pretest data and stratifies the allocation of treatments to experimental units on the predicted dependent variables. This approach reduces much of the arbitrariness involved in defining strata directly on the basis of co-variates. A simulation on 300 random samples drawn from six data sets shows that this algorithm is extremely effective in reducing the variance of the estimation compared to random allocation and to traditional ways of stratification. On average, this stratification approach requires half the sample size to estimate the treatment effect with the same precision as complete randomization.},
author = {Aufenanger, Tobias},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/898624746.pdf:pdf},
issn = {1867-6707},
keywords = {C15,C90 Keywords: experiment design,JEL Classification: C14,treatment allocation},
title = {{Machine Learning to Improve Experimental Design}},
url = {https://www.iwf.rw.fau.de/research/iwf-discussion-paper-series/},
year = {2017}
}
@article{Bai2019,
author = {Bai, Yuehao},
doi = {10.2139/ssrn.3483834},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/SSRN-id3483834.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{Optimality of Matched-Pair Designs in Randomized Controlled Trials}},
year = {2019}
}
@article{Barrios2014,
abstract = {This paper shows that stratifying on the conditional expectation of the outcome given baseline variables is optimal in matched-pair randomized experiments. The assign- ment minimizes the variance of the post-treatment difference in mean outcomes between treatment and controls. Optimal pairing depends only on predicted values of outcomes for experimental units, where the predicted values are the conditional expectations. After randomization, both frequentist inference and randomization inference depend only on the actual strata chosen and not on estimated predicted values. This gives experimenters away to use big data (possibly more covariates than the number of experimental units) ex-ante while maintaining simple post-experiment inference techniques. Optimizing the random- ization with respect to one outcome allows researchers to credibly signal the outcome of interest prior to the experiment. Inference can be conducted in the standard way by re- gressing the outcome on treatment and strata indicators. We illustrate the application of the methodology by running simulations based on a set of field experiments. We find that optimal designs have mean squared errors 23% less than randomized designs, on average. In one case, mean squared error is 43% less than randomized designs.},
author = {Barrios, Thomas},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/opstratv17_0.pdf:pdf},
pages = {1--70},
title = {{Optimal Stratification in Randomized Experiments}},
url = {http://scholar.harvard.edu/files/tbarrios/files/opstratv17_0.pdf},
year = {2014}
}
@article{Berry2004,
abstract = {The Bayesian approach is being used increasingly in medical research. The flexibility of the Bayesian approach allows for building designs of clinical trials that have good properties of any desired sort. Examples include maximizing effective treatment of patients in the trial, maximizing information about the slope of a dose-response curve, minimizing costs, minimizing the number of patients treated, minimizing the length of the trial and combinations of these desiderata. They also include standard frequentist operating characteristics when these are important considerations. Posterior probabilities are updated via Bayes' theorem on the basis of accumulating data. These are used to effect modifications of the trial's course, including stopping accrual, extending accrual beyond that originally planned, dropping treatment arms, adding arms, etc. An important aspect of the approach I advocate is modeling the relationship between a trial's primary endpoint and early indications of patient performance - auxiliary endpoints. This has several highly desirable consequences. One is that it improves the efficiency of adaptive trials because information is available sooner than otherwise.},
author = {Berry, Donald A.},
doi = {10.1214/088342304000000044},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/euclid.ss.1089808281.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Adaptive designs,Auxiliary end-points,Bayesian updating,Clinical ethics,Clinical trials,Decision analysis,Extraim analyses,Predictive probabilities},
number = {1},
pages = {175--187},
title = {{Bayesian statistics and the efficiency and ethics of clinical trials}},
volume = {19},
year = {2004}
}
@article{Breza,
author = {Breza, Emily},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/w23491.pdf:pdf},
title = {{USING AGGREGATED RELATIONAL DATA TO FEASIBLY IDENTIFY NETWORK STRUCTURE WITHOUT NETWORK DATA}}
}
@article{Brown2008,
abstract = {How to design an efficient large-area survey continues to be an interesting question for ecologists. In sampling large areas, as is common in environmental studies, adaptive sampling can be efficient because it ensures survey effort is targeted to subareas of high interest. In two-stage sampling, higher density primary sample units are usually of more interest than lower density primary units when populations are rare and clustered. Two-stage sequential sampling has been suggested as a method for allocating second stage sample effort among primary units. Here, we suggest a modification: adaptive two-stage sequential sampling. In this method, the adaptive part of the allocation process means the design is more flexible in how much extra effort can be directed to higher-abundance primary units. We discuss how best to design an adaptive two-stage sequential sample. {\textcopyright} 2008 The Society of Population Ecology and Springer.},
author = {Brown, Jennifer A. and {Salehi M.}, Mohammad and Moradi, Mohammad and Bell, Gavin and Smith, David R.},
doi = {10.1007/s10144-008-0089-1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Brown2008_Article_AnAdaptiveTwo-stageSequentialD.pdf:pdf},
isbn = {8415683111},
issn = {14383896},
journal = {Population Ecology},
keywords = {Murthy estimator,Optimal allocation,Rare clustered populations},
number = {3},
pages = {239--245},
title = {{An adaptive two-stage sequential design for sampling rare and clustered populations}},
volume = {50},
year = {2008}
}
@article{Caria2020,
abstract = {We introduce a novel methodology for adaptive targeted experiments. Our Tempered Thompson Algorithm balances the goals of maximizing the precision of treatment effect estimates and maximizing the welfare of experimental participants. A hierarchical Bayesian model allows us to adaptively target treatments at different groups. We implement our methodology in a field experiment. We examine the impact of three interventions designed to tackle credit constraints, information frictions and self-control challenges on formal employment outcomes of Syrian refugees and local jobseekers in Jordan. Six weeks after treatment, we find that treatments have had minimal effect on formal employment of refugees or locals. In the next draft of this paper, we will analyze longer-term employment and well-being outcomes and discuss further applications of adaptive targeted field experiments in economic development.},
author = {Caria, Stefano and Gordon, Grant and Kasy, Maximilian and Quinn, Simon and Shami, Soha and Teytelboym, Alexander},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/RefugeesWork.pdf:pdf},
journal = {Working Paper},
title = {{An Adaptive Targeted Field Experiment: Job Search Assistance for Refugees in Jordan}},
year = {2020}
}
@article{Carneiro2020,
abstract = {In a randomized control trial, the precision of an average treatment effect estimator and the power of the corresponding t-test can be improved either by collecting data on additional individuals, or by collecting additional covariates that predict the outcome variable. To design the experiment, a researcher needs to solve this trade-off subject to her budget constraint. We show that this optimization problem is equivalent to optimally predicting outcomes by the covariates, which in turn can be solved using existing machine learning techniques using pre-experimental data such as other similar studies, a census, or a household survey. In two empirical applications, we show that our procedure can lead to reductions of up to 58% in the costs of data collection, or improvements of the same magnitude in the precision of the treatment effect estimator.},
archivePrefix = {arXiv},
arxivId = {1603.03675},
author = {Carneiro, Pedro and Lee, Sokbae and Wilhelm, Daniel},
doi = {10.1093/ectj/utz020},
eprint = {1603.03675},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/utz020.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
keywords = {data collection,machine learning,randomized control trials},
number = {1},
pages = {1--31},
title = {{Optimal data collection for randomized control trials}},
volume = {23},
year = {2020}
}
@article{Carpentier2011,
abstract = {We consider the problem of stratified sampling for Monte-Carlo integration. We model this problem in a multi-armed bandit setting, where the arms represent the strata, and the goal is to estimate a weighted average of the mean values of the arms. We propose a strategy that samples the arms according to an upper bound on their standard deviations and compare its estimation quality to an ideal allocation that would know the standard deviations of the strata. We provide two regret analyses: a distributiondependent bound {\~{O}}(n -3/2) that depends on a measure of the disparity of the strata, and a distribution-free bound {\~{O}} (n -4/3) that does not.},
author = {Carpentier, Alexandra and Munos, R{\'{e}}mi},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/4225-finite-time-analysis-of-stratified-sampling-for-monte-carlo.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
pages = {1--9},
title = {{Finite-time analysis of stratified sampling for Monte Carlo}},
year = {2011}
}
@article{Carpentier2015,
abstract = {We consider the problem of stratified sampling for Monte Carlo integration of a random variable. We model this problem in a K-armed bandit, where the arms represent the K strata. The goal is to estimate the integral mean, that is a weighted average of the mean values of the arms. The learner is allowed to sample the variable n times, but it can decide on-line which stratum to sample next. We propose an UCB-type strategy that samples the arms according to an upper bound on their estimated standard deviations. We compare its performance to an ideal sample allocation that knows the standard deviations of the arms. For sub-Gaussian arm distributions, we provide bounds on the total regret: a distributiondependent bound of order poly($\lambda$min-1)O (n-3,2)1 that depends on a measure of the disparity $\lambda$min of the per stratum variances and a distribution-free bound poly(K)O (n-7/6) that does not. We give similar, but somewhat sharper bounds on a proxy of the regret. The problemindependent bound for this proxy matches its recent minimax lower bound in terms of n up to a log n factor.},
author = {Carpentier, Alexandra and Munos, Remi and Antos, Andr{\'{a}}s},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/carpentier15a.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Active learning,Adaptive sampling,Bandit theory,Minimax strategies,Stratified Monte Carlo},
pages = {2231--2271},
title = {{Adaptive strategy for stratified Monte Carlo sampling}},
volume = {16},
year = {2015}
}
@article{Chaloner1995,
author = {Chaloner, Kathryn and Verdinelli, Isabella},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2246015.pdf:pdf},
keywords = {and phrases,decision theory,gistic regression,hierarchical linear models,lo-,nonlinear design,nonlinear models,optimal design,optimality criteria,utility functions},
title = {{Bayesian Experimental Design : A Review}},
year = {1995}
}
@article{Etore2011,
author = {Etore, Pierre and Fort, Gersende and Jourdain, Benjamin and Moulines, Eric},
doi = {10.1007/s10479-009-0638-9},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/aorpubli.pdf:pdf},
issn = {0254-5330},
journal = {Annals of Operations Research},
number = {1},
pages = {127--154},
title = {{On adaptive stratification}},
url = {http://link.springer.com/10.1007/s10479-009-0638-9},
volume = {189},
year = {2011}
}

@article{Handcock2012,
abstract = {Network models are widely used to represent relational information among interacting units and the structural implications of these relations. Recently, social network studies have focused a great deal of attention on random graph models of networks whose nodes represent individual social actors and whose edges represent a specified relationship between the actors. Most inference for social network models assumes that the presence or absence of all possible links is observed, that the information is completely reliable, and that there are no measurement (e.g., recording) errors. This is clearly not true in practice, as much network data is collected though sample surveys. In addition even if a census of a population is attempted, individuals and links between individuals are missed (i.e., do not appear in the recorded data). In this paper we develop the conceptual and computational theory for inference based on sampled network information. We first review forms of network sampling designs used in practice. We consider inference from the likelihood framework, and develop a typology of network data that reflects their treatment within this frame. We then develop inference for social network models based on information from adaptive network designs. We motivate and illustrate these ideas by analyzing the effect of link-tracing sampling designs on a collaboration network. {\textcopyright} 2010 Institute of Mathematical Statistics.},
author = {Handcock, Mark S. and Gile, Krista J.},
doi = {10.1214/08-AOAS221},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/euclid.aoas.1273584445.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Design-based inference,Exponential family random graph model,Markov chain Monte Carlo,P model},
number = {1},
pages = {5--25},
title = {{Modeling social networks from sampled data}},
volume = {6},
year = {2012}
}
@article{Imai2009,
abstract = {A basic feature of many field experiments is that investigators are only able to randomize clusters of individuals-such as households, communities, firms, medical practices, schools or classrooms-even when the individual is the unit of interest. To recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, many other studies avoid pairing, in part because of claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. We argue that all such claims are unfounded. We also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; its standard error is also invalid. To overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. We also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. Our methods also address individual-level noncompliance, which is common in applications but not allowed for in most existing methods. We show that from the perspective of bias, efficiency, power, robustness or research costs, and in large or small samples, pairing should be used in cluster-randomized experiments whenever feasible; failing to do so is equivalent to discarding a considerable fraction of one's data. We develop these techniques in the context of a randomized evaluation we are conducting of the Mexican Universal Health Insurance Program. {\textcopyright} Institute of Mathematical Statistics, 2009.},
author = {Imai, Kosuke and King, Gary and Nall, Clayton},
doi = {10.1214/08-STS274},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/euclid.ss.1255009008.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Causal inference,Community intervention trials,Field experiments,Group-randomized trials,Health policy,Matched-pair design,Noncompliance,Place-randomized trials,Power},
number = {1},
pages = {29--53},
title = {{The essential role of pair matching in cluster-randomized experiments, with application to the Mexican Universal Health Insurance Evaluation}},
volume = {24},
year = {2009}
}
@article{Jankowski2015,
abstract = {The uniqueness of online social networks makes it possible to implement new methods that increase the quality and effectiveness of research processes. While surveys are one of the most important tools for research, the representativeness of selected online samples is often a challenge and the results are hardly generalizable. An approach based on surveys with representativeness targeted at network measure distributions is proposed and analyzed in this paper. Its main goal is to focus not only on sample representativeness in terms of demographic attributes, but also to follow the measures distributions within main network. The approach presented has many application areas related to online research, sampling a network for the evaluation of collaborative learning processes, and candidate selection for training purposes with the ability to distribute information within a social network.},
author = {Jankowski, Jaros{\l}aw and Michalski, Rados{\l}aw and Br{\'{o}}dka, Piotr and Kazienko, Przemys{\l}aw and Utz, Sonja},
doi = {10.1016/j.chb.2014.12.015},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1505.03049.pdf:pdf},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Adaptive surveys,Collaborative learning,Network sampling,Social network analysis},
pages = {685--693},
title = {{Knowledge acquisition from social platforms based on network distributions fitting}},
volume = {51},
year = {2015}
}
@article{Keeter2017,
abstract = {Telephone polls still provide accurate data on a wide range of social, demographic and political variables, but some weaknesses persist.},
author = {Keeter, Scott and Hatley, Nick and Kennedy, Courtney and Lau, Arnold},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/RDD-Non-response-Full-Report.pdf:pdf},
pages = {1--39},
title = {{What Low Response Rates Mean for Telephone Surveys}},
url = {http://www.pewresearch.org/wp-content/uploads/2017/05/RDD-Non-response-Full-Report.pdf},
year = {2017}
}
@book{Kivinen2011,
author = {Kivinen, Jyriki and Szepesv{\'{a}}ri, Csaba and Ukkonen, Esko and Zeugmann, Thomas},
doi = {10.1007/978-3-642-24412-4},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2011_Book_AlgorithmicLearningTheory.pdf:pdf},
isbn = {9783642244124},
title = {{Algorithmic Learning Theory: 22nd International Conference, ALT 2011, Espoo, Finland, October 5-7, 2011, Proceedings}},
url = {http://books.google.com/books?hl=en&lr=&id=T8UVPK_5_3cC&oi=fnd&pg=PP2&dq=%22Conference+on+Discovery+Science+(DS+2011).+The+technical+program%22+%22on-line+learning+and+relative+loss+bounds,+semi-supervised+and%22+%22on+the+development+and+analysis+of+meth},
volume = {6925},
year = {2011}
}
@article{Moore2012,
abstract = {Political scientists use randomized treatment assignments to aid causal inference in field experiments, psychological laboratories, and survey research. Political research can do considerably better than completely randomized designs, but few political science experiments combine random treatment assignment with blocking on a rich set of background covariates. We describe high-dimensional multivariate blocking, including on continuous covariates, detail its statistical and political advantages over complete randomization, introduce a particular algorithm, and propose a procedure to mitigate unit interference in experiments. We demonstrate the performance of our algorithm in simulations and three field experiments from campaign politics and education. {\textcopyright} The Author 2012. Published by Oxford University Press on behalf of the Society for Political Methodology. All rights reserved.},
author = {Moore, Ryan T.},
doi = {10.1093/pan/mps025},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Political Analysis Volume 20 issue 4 2012 [doi 10.1093_pan_mps025] Moore, R. T. -- Multivariate Continuous Blocking to Improve Political Science Experiments.pdf:pdf},
issn = {10471987},
journal = {Political Analysis},
number = {4},
pages = {460--479},
title = {{Multivariate continuous blocking to improve political science experiments}},
volume = {20},
year = {2012}
}
@article{Park2004,
abstract = {We fit a multilevel logistic regression model for the mean of a binary response variable conditional on poststratification cells. This approach combines the modeling approach often used in small-area estimation with the population information used in poststratification (see Gelman and Little 1997, Survey Methodology 23:127-135). To validate the method, we apply it to U.S. preelection polls for 1988 and 1992, poststratified by state, region, and the usual demographic variables. We evaluate the model by comparing it to state-level election outcomes. The multilevel model outperforms more commonly used models in political science. We envision the most important usage of this method to be not forecasting elections but estimating public opinion on a variety of issues at the state level. {\textcopyright} Society for Political Methodology 2004; all rights reserved.},
author = {Park, David K. and Gelman, Andrew and Bafumi, Joseph},
doi = {10.1093/pan/mph024},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/parkgelmanbafumi.pdf:pdf},
issn = {10471987},
journal = {Political Analysis},
number = {4},
pages = {375--385},
title = {{Bayesian multilevel estimation with poststratification: State-level estimates from national polls}},
volume = {12},
year = {2004}
}
@article{Peter,
author = {Peter, M and Berry, Don A and Grieve, Andrew P and Krams, Michael},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/MBGK06.pdf:pdf},
pages = {1--19},
title = {{A Bayesian Decision-Theoretic Dose Finding Trial}}
}
@article{Processes2019,
author = {Processes, Hydrological},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/BIOMTRKA-20-503-Proof-hi.pdf:pdf},
keywords = {approximations of operating characteristics,central limit theorem,directed trial designs,large sample,stochastic approximation},
title = {{Fo r P ee r R ev iew Fo r P modeling r R}},
year = {2019}
}
@article{Russo2018,
abstract = {We propose information-directed sampling-a new approach to online optimization problems in which a decision maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. We illustrate through simple analytic examples how information-directed sampling accounts for kinds of information that alternative approaches do not adequately address and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate state-of-the-art simulation performance.},
archivePrefix = {arXiv},
arxivId = {1403.5556},
author = {Russo, Daniel and {Van Roy}, Benjamin},
doi = {10.1287/opre.2017.1663},
eprint = {1403.5556},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1403.5556.pdf:pdf},
issn = {15265463},
journal = {Operations Research},
keywords = {Exploration/exploitation,Information theory,Multi-armed bandit,Online optimization},
number = {1},
pages = {230--252},
title = {{Learning to optimize via information-directed sampling}},
volume = {66},
year = {2018}
}
@article{Salehi2010,
abstract = {In stratified sampling, methods for the allocation of effort among strata usually rely on some measure of within-stratum variance. If we do not have enough information about these variances, adaptive allocation can be used. In adaptive allocation designs, surveys are conducted in two phases. Information from the first phase is used to allocate the remaining units among the strata in the second phase. Brown et al. [Adaptive two-stage sequential sampling, Popul. Ecol. 50 (2008), pp. 239-245] introduced an adaptive allocation sampling design - where the final sample size was random - and an unbiased estimator. Here, we derive an unbiased variance estimator for the design, and consider a related design where the final sample size is fixed. Having a fixed final sample size can make survey-planning easier. We introduce a biased Horvitz-Thompson type estimator and a biased sample mean type estimator for the sampling designs.We conduct two simulation studies on honey producers in Kurdistan and synthetic zirconium distribution in a region on the moon. Results show that the introduced estimators are more efficient than the available estimators for both variable and fixed sample size designs, and the conventional unbiased estimator of stratified simple random sampling design. In order to evaluate efficiencies of the introduced designs and their estimator furthermore, we first review some well-known adaptive allocation designs and compare their estimator with the introduced estimators. Simulation results show that the introduced estimators are more efficient than available estimators of these well-known adaptive allocation designs. {\textcopyright} 2010 Taylor & Francis.},
author = {Salehi, Mohammad and Moradi, Mohammad and Brown, Jennifer A. and Smith, David},
doi = {10.1080/00949650903005664},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Journal of Statistical Computation and Simulation Volume 80 issue 10 2010 [doi 10.1080_00949650903005664] Salehi, Mohammad\; Moradi, Mohammad\; Brown, Jennifer A.\; Smith, D -- Efficient estimators for.pdf:pdf},
isbn = {8415683111},
issn = {00949655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Adaptive allocation,Horvitz-thompson type estimator,Neyman's allocation,Sample mean type estimator},
number = {10},
pages = {1163--1179},
title = {{Efficient estimators for adaptive stratified sequential sampling}},
volume = {80},
year = {2010}
}
@article{Schneider2019,
abstract = {In this article, we explore the use of Facebook targeted advertisements for the collection of survey data. We illustrate the potential of survey sampling and recruitment on Facebook through the example of building a large employee–employer linked data set as part of The Shift Project. We describe the workflow process of targeting, creating, and purchasing survey recruitment advertisements on Facebook. We address concerns about sample selectivity and apply poststratification weighting techniques to adjust for differences between our sample and that of “gold standard” data sources. We then compare univariate and multivariate relationships in the Shift data against the Current Population Survey and the National Longitudinal Survey of Youth 1997. Finally, we provide an example of the utility of the firm-level nature of the data by showing how firm-level gender composition is related to wages. We conclude by discussing some important remaining limitations of the Facebook approach, as well as highlighting some unique strengths of the Facebook targeted advertisement approach, including the ability for rapid data collection in response to research opportunities, rich and flexible sample targeting capabilities, and low cost, and we suggest broader applications of this technique.},
author = {Schneider, Daniel and Harknett, Kristen},
doi = {10.1177/0049124119882477},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/0049124119882477.pdf:pdf},
issn = {15528294},
journal = {Sociological Methods and Research},
keywords = {firm-level data,gender,nonprobability sampling,survey methods,work},
title = {{What's to Like? Facebook as a Tool for Survey Data Collection}},
year = {2019}
}
@book{Seber,
author = {Seber, George A.F. and Salehi, Mohammad M.},
doi = {10.1007/978-3-642-33657-7},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/[SpringerBriefs in Statistics] George A.F. Seber, Mohammad M. Salehi (auth.) - Adaptive Sampling Designs_ Inference for Sparse and Clustered Populations (2013, Springer-Verlag Berlin Heidelberg) - libgen.lc.pdf:pdf},
isbn = {978-3-642-33656-0},
publisher = {Springer Berlin Heidelberg},
series = {SpringerBriefs in Statistics},
title = {{Adaptive Sampling Designs}},
url = {http://www.springer.com/series/8921 http://link.springer.com/10.1007/978-3-642-33657-7}
}
@article{Shekhar2019,
abstract = {We consider the problem of allocating samples to a finite set of discrete distributions in order to learn them uniformly well in terms of four common distance measures: $\ell_2^2$, $\ell_1$, $f$-divergence, and separation distance. To present a unified treatment of these distances, we first propose a general optimistic tracking algorithm and analyze its sample allocation performance w.r.t.$\sim$an oracle. We then instantiate this algorithm for the four distance measures and derive bounds on the regret of their resulting allocation schemes. We verify our theoretical findings through some experiments. Finally, we show that the techniques developed in the paper can be easily extended to the related setting of minimizing the average error (in terms of the four distances) in learning a set of distributions.},
archivePrefix = {arXiv},
arxivId = {1910.12406},
author = {Shekhar, Shubhanshu and Javidi, Tara and Ghavamzadeh, Mohammad},
eprint = {1910.12406},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/ICML-2020-adaptive-sampling-for-estimating-probability-distributions-Paper.pdf:pdf},
number = {2013},
title = {{Adaptive Sampling for Estimating Multiple Probability Distributions}},
url = {http://arxiv.org/abs/1910.12406},
year = {2019}
}
@article{Tabord-Meehan2020,
abstract = {This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, as well as the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, the results we present are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure ,can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment.},
archivePrefix = {arXiv},
arxivId = {1806.05127},
author = {Tabord-Meehan, Max},
eprint = {1806.05127},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1806.05127.pdf:pdf},
keywords = {adaptive randomization,c14,c21,c93,decision trees,jel classification codes,randomized experiments},
title = {{Stratification Trees for Adaptive Randomization in Randomized Controlled Trials}},
url = {http://arxiv.org/abs/1806.05127},
year = {2020}
}
@article{Thompson2011,
abstract = {This paper describes recent developments in adaptive sampling strategies and introduces new variations on those strategies. Recent developments described included targeted random walk designs and adaptive web sampling. These designs are particularly suited for sampling in networks; for example, for finding a sample of people from a hidden human population by following social links from sample individuals to find additional members of the hidden population to add to the sample. Each of these designs can also be translated into spatial settings to produce flexible new spatial adaptive strategies for sampling unevenly distributed populations. Variations on these sampling strategies include versions in which the network or spatial links have unequal weights and are followed with unequal probabilities.},
author = {Thompson, Steve},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/11607-eng.pdf:pdf},
issn = {07140045},
journal = {Survey Methodology},
keywords = {Adaptive web sampling,Markov chain,Network sampling,Random walk,Snowball sampling},
number = {2},
pages = {183--196},
title = {{Adaptive network and spatial sampling}},
volume = {37},
year = {2011}
}
@article{Viviano2019,
abstract = {This paper discusses the problem of estimating individualized treatment allocation rules under network interference. We propose a method with several appealing features for applications: we let treatment and spillover effects be heterogeneous in the population, and we construct targeting rules that exploit such heterogeneity; we accommodate for arbitrary, possibly non-linear, regression models, and we propose estimators that are robust to model misspecification; treatment allocation rules depend on an arbitrary set of individual, neighbors' and network characteristics, and we allow for general constraints on the policy function and capacity constraints on the number of treated units; the proposed methodology is valid also when only local information of the network is observed. From a theoretical perspective, we establish the first set of guarantees on the utilitarian regret under interference, and we show that it achieves the min-max optimal rate in scenarios of practical and theoretical interest. We provide a mixed-integer linear program formulation of the optimization problem, that can be solved using standard optimization routines. We discuss the empirical performance in simulations, and we illustrate our method by investigating the role of social networks in micro-finance decisions.},
archivePrefix = {arXiv},
arxivId = {1906.10258},
author = {Viviano, Davide},
eprint = {1906.10258},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2003.08421.pdf:pdf},
pages = {1--55},
title = {{Policy Targeting under Network Interference}},
url = {http://arxiv.org/abs/1906.10258},
year = {2019}
}

@article{Wang2015,
abstract = {Election forecasts have traditionally been based on representative polls, in which randomly sampled individuals are asked who they intend to vote for. While representative polling has historically proven to be quite effective, it comes at considerable costs of time and money. Moreover, as response rates have declined over the past several decades, the statistical benefits of representative sampling have diminished. In this paper, we show that, with proper statistical adjustment, non-representative polls can be used to generate accurate election forecasts, and that this can often be achieved faster and at a lesser expense than traditional survey methods. We demonstrate this approach by creating forecasts from a novel and highly non-representative survey dataset: a series of daily voter intention polls for the 2012 presidential election conducted on the Xbox gaming platform. After adjusting the Xbox responses via multilevel regression and poststratification, we obtain estimates which are in line with the forecasts from leading poll analysts, which were based on aggregating hundreds of traditional polls conducted during the election cycle. We conclude by arguing that non-representative polling shows promise not only for election forecasting, but also for measuring public opinion on a broad range of social, economic and cultural issues.},
author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
doi = {10.1016/j.ijforecast.2014.06.001},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/International Journal of Forecasting Volume 31 issue 3 2015 [doi 10.1016_j.ijforecast.2014.06.001] Wang, Wei\; Rothschild, David\; Goel, Sharad\; Gelman, Andrew -- Forecasting elections with non-repres.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Election forecasting,Multilevel regression and poststratification,Non-representative polling},
number = {3},
pages = {980--991},
publisher = {Elsevier B.V.},
title = {{Forecasting elections with non-representative polls}},
volume = {31},
year = {2015}
}

@article{Zacks1969,
abstract = {The problem of choosing a sample of a fixed size from a finite population is studied in the framework of Bayes designs. The conjectured non-randomized character of Bayes designs is verified. Bayes designs are shown to be without replacement selections, which are generally sequential ones. Sufficient conditions are provided for the optimality of single-phase designs. An example is provided of a case in which the optimal Bayes design is sequential. {\textcopyright} Taylor & Francis Group, LLC.},
author = {Zacks, Shelley},
doi = {10.1080/01621459.1969.10501060},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Journal of the American Statistical Association Volume 64 issue 328 1969 [doi 10.1080_01621459.1969.10501060] Zacks, Shelley -- Bayes Sequential Designs of Fixed Size Samples from Finite Populations.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {328},
pages = {1342--1349},
title = {{Bayes Sequential Designs of Fixed Size Samples from Finite Populations}},
volume = {64},
year = {1969}
}
@article{Zagheni2017,
author = {Zagheni, Emilio and Weber, Ingmar and Gummadi, Krishna},
doi = {10.1111/padr.12102},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/padr.12102.pdf:pdf},
issn = {17284457},
journal = {Population and Development Review},
number = {4},
pages = {721--734},
title = {Leveraging {Facebook's} Advertising Platform to Monitor Stocks of Migrants},
volume = {43},
year = {2017}
}
